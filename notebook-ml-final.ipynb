{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":81933,"databundleVersionId":9643020,"sourceType":"competition"},{"sourceId":7453542,"sourceType":"datasetVersion","datasetId":921302}],"dockerImageVersionId":30805,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true},"papermill":{"default_parameters":{},"duration":559.18263,"end_time":"2024-12-09T08:43:56.381032","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-12-09T08:34:37.198402","version":"2.6.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip -q install /kaggle/input/pytorchtabnet/pytorch_tabnet-4.1.0-py3-none-any.whl","metadata":{"papermill":{"duration":41.658126,"end_time":"2024-12-09T08:35:21.527928","exception":false,"start_time":"2024-12-09T08:34:39.869802","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport re\nimport random\nfrom copy import deepcopy\nfrom sklearn.base import clone, BaseEstimator, RegressorMixin, TransformerMixin\nfrom sklearn.metrics import cohen_kappa_score\nfrom sklearn.model_selection import StratifiedKFold, train_test_split\nfrom sklearn.decomposition import PCA\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\n\nfrom scipy.optimize import minimize\nfrom concurrent.futures import ThreadPoolExecutor\nfrom tqdm import tqdm\n\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\nfrom keras.models import Model\nfrom keras.layers import Input, Dense\nfrom keras.optimizers import Adam\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom pytorch_tabnet.tab_model import TabNetRegressor\nfrom pytorch_tabnet.callbacks import Callback\n\nfrom colorama import Fore, Style\nfrom IPython.display import clear_output\nimport warnings\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\nfrom catboost import CatBoostRegressor\nfrom pytorch_tabnet.tab_model import TabNetRegressor\nfrom pytorch_tabnet.callbacks import Callback\nfrom sklearn.ensemble import VotingRegressor, RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.impute import SimpleImputer, KNNImputer\nfrom sklearn.pipeline import Pipeline\nimport seaborn as sns\nwarnings.filterwarnings('ignore')\npd.options.display.max_columns = None\n\nSEED = 18\nn_splits = 5","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":20.91073,"end_time":"2024-12-09T08:35:42.443632","exception":false,"start_time":"2024-12-09T08:35:21.532902","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Hàm xử lý time-series\n\n### Mô tả mã nguồn\n\n- **`process_file(filename, dirname)`**:  \n  Hàm này đọc từng file dữ liệu time-series trong định dạng Parquet, xóa cột không cần thiết (`step`), và tính toán các thống kê mô tả như trung bình, phương sai, tối đa, tối thiểu, v.v. Kết quả trả về gồm:  \n  1. **Các giá trị thống kê** dưới dạng mảng.  \n  2. **ID của file**, được tách từ tên file.\n\n- **`load_time_series(dirname)`**:  \n  Hàm này xử lý toàn bộ thư mục chứa dữ liệu time-series bằng cách:\n  1. Lấy danh sách tất cả các file trong thư mục (`ids`).  \n  2. Sử dụng `ThreadPoolExecutor` để xử lý đa luồng, gọi `process_file` trên từng file, tăng tốc độ xử lý.  \n  3. Kết hợp các kết quả từ tất cả file thành một DataFrame. Mỗi hàng tương ứng với một file, các cột là các giá trị thống kê được gán nhãn như `stat_0`, `stat_1`,...  \n  4. Thêm cột `id` để lưu trữ ID tương ứng với mỗi hàng.\n\n### Ý nghĩa:\n- **Tự động hóa xử lý dữ liệu time-series:**  \n  Giảm kích thước dữ liệu time-series bằng cách chỉ giữ lại các đặc trưng quan trọng (thống kê), giúp tăng tốc độ xử lý và huấn luyện mô hình.\n\n- **Tăng hiệu quả tính toán:**  \n  Việc sử dụng `ThreadPoolExecutor` giúp xử lý đa luồng, tiết kiệm thời gian khi làm việc với nhiều file.\n\n- **Đầu ra:**  \n  DataFrame chứa các đặc trưng thống kê của toàn bộ dữ liệu time-series, sẵn sàng để tích hợp vào dữ liệu chính.\n","metadata":{}},{"cell_type":"code","source":"def process_file(filename, dirname):\n    df = pd.read_parquet(os.path.join(dirname, filename, 'part-0.parquet'))\n    df.drop('step', axis=1, inplace=True)\n    return df.describe().values.reshape(-1), filename.split('=')[1]\n\ndef load_time_series(dirname) -> pd.DataFrame:\n    ids = os.listdir(dirname)\n    \n    with ThreadPoolExecutor() as executor:\n        results = list(tqdm(executor.map(lambda fname: process_file(fname, dirname), ids), total=len(ids)))\n    \n    stats, indexes = zip(*results)\n    \n    df = pd.DataFrame(stats, columns=[f\"stat_{i}\" for i in range(len(stats[0]))])\n    df['id'] = indexes\n    return df\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# AutoEncoder\n\n### Mô tả\n\n#### **`class AutoEncoder`**\n- **Encoder**: Giảm chiều dữ liệu từ `input_dim` xuống `encoding_dim` qua các tầng `Linear` với hàm kích hoạt `ReLU`.\n- **Decoder**: Tái tạo dữ liệu từ `encoding_dim` về `input_dim` qua các tầng `Linear`, dùng `Sigmoid` ở tầng cuối.\n- **`forward`**: Trích xuất đặc trưng qua `encoder` và tái tạo dữ liệu qua `decoder`.\n\n#### **`perform_autoencoder`**\n- **Chuẩn hóa dữ liệu**: Chuẩn hóa bằng `StandardScaler`, chuyển đổi sang Tensor.\n- **Huấn luyện**: Sử dụng `MSELoss` để tối ưu việc tái tạo dữ liệu qua AutoEncoder, cập nhật trọng số bằng Adam.\n- **Trích xuất đặc trưng**: Lấy đầu ra từ `encoder` để tạo DataFrame các đặc trưng mã hóa (`Enc_1`, `Enc_2`,...).\n\n### Ý nghĩa\n- **Giảm chiều dữ liệu**: Tự động học đặc trưng quan trọng với ít chiều hơn.\n- **Tăng hiệu quả tính toán**: Chuẩn bị dữ liệu cho các mô hình sau.\n- **Đầu ra**: DataFrame chứa đặc trưng mã hóa, sẵn sàng sử dụng.\n","metadata":{}},{"cell_type":"code","source":"class AutoEncoder(nn.Module):\n    def __init__(self, input_dim, encoding_dim):\n        super(AutoEncoder, self).__init__()\n        self.encoder = nn.Sequential(\n            nn.Linear(input_dim, encoding_dim*3),\n            nn.ReLU(),\n            nn.Linear(encoding_dim*3, encoding_dim*2),\n            nn.ReLU(),\n            nn.Linear(encoding_dim*2, encoding_dim),\n            nn.ReLU()\n        )\n        self.decoder = nn.Sequential(\n            nn.Linear(encoding_dim, input_dim*2),\n            nn.ReLU(),\n            nn.Linear(input_dim*2, input_dim*3),\n            nn.ReLU(),\n            nn.Linear(input_dim*3, input_dim),\n            nn.Sigmoid()\n        )\n        \n    def forward(self, x):\n        encoded = self.encoder(x)\n        decoded = self.decoder(encoded)\n        return decoded\n    \ndef perform_autoencoder(df, encoding_dim=50, epochs=50, batch_size=32):\n    scaler = StandardScaler()\n    df_scaled = scaler.fit_transform(df)\n    \n    data_tensor = torch.FloatTensor(df_scaled)\n    \n    input_dim = data_tensor.shape[1]\n    autoencoder = AutoEncoder(input_dim, encoding_dim)\n    \n    criterion = nn.MSELoss()\n    optimizer = optim.Adam(autoencoder.parameters())\n    \n    for epoch in range(epochs):\n        for i in range(0, len(data_tensor), batch_size):\n            batch = data_tensor[i : i + batch_size]\n            optimizer.zero_grad()\n            reconstructed = autoencoder(batch)\n            loss = criterion(reconstructed, batch)\n            loss.backward()\n            optimizer.step()\n            \n        if (epoch + 1) % 10 == 0:\n            print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}]')\n                 \n    with torch.no_grad():\n        encoded_data = autoencoder.encoder(data_tensor).numpy()\n        \n    df_encoded = pd.DataFrame(encoded_data, columns=[f'Enc_{i + 1}' for i in range(encoded_data.shape[1])])\n    \n    return df_encoded","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Feature engineering","metadata":{}},{"cell_type":"code","source":"def feature_engineering(df):\n    season_cols = [col for col in df.columns if 'Season' in col]\n    df = df.drop(season_cols, axis=1) \n    # Create engineered features\n    df['BMI_Age'] = df['Physical-BMI'] * df['Basic_Demos-Age']\n    df['Internet_Hours_Age'] = df['PreInt_EduHx-computerinternet_hoursday'] * df['Basic_Demos-Age']\n    df['BMI_Internet_Hours'] = df['Physical-BMI'] * df['PreInt_EduHx-computerinternet_hoursday']\n    df['BFP_BMI'] = df['BIA-BIA_Fat'] / df['BIA-BIA_BMI']\n    df['FFMI_BFP'] = df['BIA-BIA_FFMI'] / df['BIA-BIA_Fat']\n    df['FMI_BFP'] = df['BIA-BIA_FMI'] / df['BIA-BIA_Fat']\n    df['LST_TBW'] = df['BIA-BIA_LST'] / df['BIA-BIA_TBW']\n    df['BFP_BMR'] = df['BIA-BIA_Fat'] * df['BIA-BIA_BMR']\n    df['BFP_DEE'] = df['BIA-BIA_Fat'] * df['BIA-BIA_DEE']\n    df['BMR_Weight'] = df['BIA-BIA_BMR'] / df['Physical-Weight']\n    df['DEE_Weight'] = df['BIA-BIA_DEE'] / df['Physical-Weight']\n    df['SMM_Height'] = df['BIA-BIA_SMM'] / df['Physical-Height']\n    df['Muscle_to_Fat'] = df['BIA-BIA_SMM'] / df['BIA-BIA_FMI']\n    df['Hydration_Status'] = df['BIA-BIA_TBW'] / df['Physical-Weight']\n    df['ICW_TBW'] = df['BIA-BIA_ICW'] / df['BIA-BIA_TBW']\n    \n    return df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Hàm Feature Engineering\n\n### Mô tả\n\n#### **Xử lý dữ liệu**\n- **Xóa các cột mùa (`Season`)**: Loại bỏ thông tin mùa vì không cần thiết cho mô hình.\n\n#### **Tạo đặc trưng mới**\n- **Kết hợp các cột hiện có để tạo đặc trưng ý nghĩa hơn:**\n  - `BMI_Age`: Kết hợp giữa chỉ số BMI và tuổi.\n  - `Internet_Hours_Age`: Số giờ sử dụng internet nhân với tuổi.\n  - `BMI_Internet_Hours`: Tích hợp BMI và thời gian sử dụng internet.\n  - Tỷ lệ:\n    - `BFP_BMI`: Tỷ lệ phần trăm mỡ cơ thể (BFP) so với BMI.\n    - `FFMI_BFP`, `FMI_BFP`: Các chỉ số về cơ và mỡ.\n    - `LST_TBW`: Tỷ lệ cơ xương nạc trên tổng lượng nước cơ thể.\n  - Tích: \n    - `BFP_BMR`, `BFP_DEE`: Liên kết BFP với chuyển hóa cơ bản (BMR) và năng lượng tiêu hao (DEE).\n  - Tỷ lệ khác:\n    - `BMR_Weight`, `DEE_Weight`: Điều chỉnh BMR và DEE theo trọng lượng.\n    - `SMM_Height`: Tỷ lệ khối cơ xương so với chiều cao.\n    - `Muscle_to_Fat`: Tỷ lệ cơ trên mỡ.\n    - `Hydration_Status`, `ICW_TBW`: Các chỉ số liên quan đến nước và cân bằng trong cơ thể.\n\n### Ý nghĩa\n- **Tăng thông tin hữu ích**: Giúp mô hình học được các mối quan hệ phức tạp hơn giữa các đặc trưng.\n- **Cải thiện hiệu năng**: Cung cấp thêm ngữ cảnh và ý nghĩa từ dữ liệu ban đầu.\n- **Đầu ra**: DataFrame với các đặc trưng mới được tạo và các cột không cần thiết đã loại bỏ.\n","metadata":{}},{"cell_type":"code","source":"# Định nghĩa hàm tính Quadratic Weighted Kappa\ndef quadratic_weighted_kappa(y_true, y_pred):\n    return cohen_kappa_score(y_true, y_pred, weights='quadratic')\n\n# Hàm làm tròn dự đoán dựa trên các ngưỡng\ndef threshold_Rounder(oof_non_rounded, thresholds):\n    return np.where(oof_non_rounded < thresholds[0], 0,\n                    np.where(oof_non_rounded < thresholds[1], 1,\n                             np.where(oof_non_rounded < thresholds[2], 2, 3)))\n\n# Hàm đánh giá dự đoán với các ngưỡng\ndef evaluate_predictions(thresholds, y_true, oof_non_rounded):\n    rounded_p = threshold_Rounder(oof_non_rounded, thresholds)\n    return -quadratic_weighted_kappa(y_true, rounded_p)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Hàm `TrainModel`\n\n### **Mô tả**\nHàm này thực hiện quá trình huấn luyện, đánh giá và tối ưu hóa mô hình.\n\n---\n\n### **Các bước chính**\n\n1. **Chuẩn bị dữ liệu**:\n   - Tách dữ liệu `train` thành `X` (đặc trưng) và `y` (nhãn `sii`).\n   - Sử dụng **Stratified K-Fold** để chia dữ liệu thành `n_splits` tập huấn luyện và kiểm tra.\n\n2. **Huấn luyện và kiểm tra**:\n   - Với mỗi fold:\n     - Huấn luyện mô hình trên tập huấn luyện (`train_idx`).\n     - Dự đoán và tính toán độ đo **QWK (Quadratic Weighted Kappa)** trên tập kiểm tra (`test_idx`).\n   - Lưu các kết quả:\n     - `oof_non_rounded`: Dự đoán không làm tròn.\n     - `oof_rounded`: Dự đoán sau khi làm tròn.\n     - `test_preds`: Dự đoán trên dữ liệu kiểm tra.\n\n3. **Tối ưu hóa ngưỡng**:\n   - Sử dụng hàm `minimize` với phương pháp **Nelder-Mead** để tối ưu hóa các ngưỡng làm tròn, giúp cải thiện điểm **QWK**.\n\n4. **Dự đoán cuối cùng**:\n   - Kết hợp các dự đoán từ các fold và áp dụng ngưỡng tối ưu để tạo **submission** cuối cùng.\n\n---\n\n### **Đầu ra**\n- **Kết quả trung bình trên các fold**:\n  - `Mean Train QWK`: Điểm QWK trung bình trên tập huấn luyện.\n  - `Mean Validation QWK`: Điểm QWK trung bình trên tập kiểm tra.\n  - `Optimized QWK SCORE`: Điểm QWK sau khi áp dụng ngưỡng tối ưu.\n\n- **Dự đoán cuối cùng**:\n  - Dự đoán `sii` trên dữ liệu kiểm tra được lưu trong DataFrame `submission`.\n\n---\n\n### **Ý nghĩa**\n- **Đảm bảo đánh giá công bằng**: Sử dụng Stratified K-Fold để đảm bảo phân phối nhãn đồng đều.\n- **Tối ưu hóa kết quả**: Áp dụng kỹ thuật tối ưu ngưỡng giúp cải thiện điểm đánh giá.\n- **Kết hợp dự đoán**: Lấy trung bình dự đoán từ các fold để tăng độ ổn định của kết quả.\n\n---","metadata":{}},{"cell_type":"code","source":"def TrainModel(model_class, test_data):\n    X = train.drop(['sii'], axis=1)\n    y = train['sii']\n\n    SKF = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n    \n    train_S = []\n    test_S = []\n    \n    oof_non_rounded = np.zeros(len(y), dtype=float) \n    oof_rounded = np.zeros(len(y), dtype=int) \n    test_preds = np.zeros((len(test_data), n_splits))\n\n    for fold, (train_idx, test_idx) in enumerate(tqdm(SKF.split(X, y), desc=\"Training Folds\", total=n_splits)):\n        X_train, X_val = X.iloc[train_idx], X.iloc[test_idx]\n        y_train, y_val = y.iloc[train_idx], y.iloc[test_idx]\n\n        model = clone(model_class)\n        model.fit(X_train, y_train)\n\n        y_train_pred = model.predict(X_train)\n        y_val_pred = model.predict(X_val)\n\n        oof_non_rounded[test_idx] = y_val_pred\n        y_val_pred_rounded = y_val_pred.round(0).astype(int)\n        oof_rounded[test_idx] = y_val_pred_rounded\n\n        train_kappa = quadratic_weighted_kappa(y_train, y_train_pred.round(0).astype(int))\n        val_kappa = quadratic_weighted_kappa(y_val, y_val_pred_rounded)\n\n        train_S.append(train_kappa)\n        test_S.append(val_kappa)\n        \n        test_preds[:, fold] = model.predict(test_data)\n        \n        print(f\"Fold {fold+1} - Train QWK: {train_kappa:.4f}, Validation QWK: {val_kappa:.4f}\")\n        clear_output(wait=True)\n\n    print(f\"Mean Train QWK --> {np.mean(train_S):.4f}\")\n    print(f\"Mean Validation QWK ---> {np.mean(test_S):.4f}\")\n\n    KappaOPtimizer = minimize(evaluate_predictions,\n                              x0=[0.5, 1.5, 2.5], args=(y, oof_non_rounded), \n                              method='Nelder-Mead')\n    assert KappaOPtimizer.success, \"Optimization did not converge.\"\n    \n    oof_tuned = threshold_Rounder(oof_non_rounded, KappaOPtimizer.x)\n    tKappa = quadratic_weighted_kappa(y, oof_tuned)\n\n    print(f\"----> || Optimized QWK SCORE :: {Fore.CYAN}{Style.BRIGHT} {tKappa:.3f}{Style.RESET_ALL}\")\n\n    tpm = test_preds.mean(axis=1)\n    tpTuned = threshold_Rounder(tpm, KappaOPtimizer.x)\n    \n    submission = pd.DataFrame({\n        'id': sample['id'],\n        'sii': tpTuned\n    })\n\n    return submission","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class AutoEncoder(nn.Module):\n    def __init__(self, input_dim, encoding_dim):\n        super(AutoEncoder, self).__init__()\n        self.encoder = nn.Sequential(\n            nn.Linear(input_dim, encoding_dim*3),\n            nn.ReLU(),\n            nn.Linear(encoding_dim*3, encoding_dim*2),\n            nn.ReLU(),\n            nn.Linear(encoding_dim*2, encoding_dim),\n            nn.ReLU()\n        )\n        self.decoder = nn.Sequential(\n            nn.Linear(encoding_dim, input_dim*2),\n            nn.ReLU(),\n            nn.Linear(input_dim*2, input_dim*3),\n            nn.ReLU(),\n            nn.Linear(input_dim*3, input_dim),\n            nn.Sigmoid()\n        )\n        \n    def forward(self, x):\n        encoded = self.encoder(x)\n        decoded = self.decoder(encoded)\n        return decoded\n    \ndef perform_autoencoder(df, encoding_dim=50, epochs=50, batch_size=32):\n    scaler = StandardScaler()\n    df_scaled = scaler.fit_transform(df)\n    \n    data_tensor = torch.FloatTensor(df_scaled)\n    \n    input_dim = data_tensor.shape[1]\n    autoencoder = AutoEncoder(input_dim, encoding_dim)\n    \n    criterion = nn.MSELoss()\n    optimizer = optim.Adam(autoencoder.parameters())\n    \n    for epoch in range(epochs):\n        for i in range(0, len(data_tensor), batch_size):\n            batch = data_tensor[i : i + batch_size]\n            optimizer.zero_grad()\n            reconstructed = autoencoder(batch)\n            loss = criterion(reconstructed, batch)\n            loss.backward()\n            optimizer.step()\n            \n        if (epoch + 1) % 10 == 0:\n            print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}]')\n                 \n    with torch.no_grad():\n        encoded_data = autoencoder.encoder(data_tensor).numpy()\n        \n    df_encoded = pd.DataFrame(encoded_data, columns=[f'Enc_{i + 1}' for i in range(encoded_data.shape[1])])\n    \n    return df_encoded","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Bảng phân phối giá trị của Sii","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/train.csv')\nmissing_count = train['sii'].isna().sum()\ntrain['sii_filled'] = train['sii'].fillna('Missing')\npercentages = train['sii_filled'].value_counts(normalize=True) * 100\nax = sns.countplot(data=train, x='sii_filled', order=percentages.index)\nax.set_title(\"Count and Percentage of 'sii' (Including Missing Values)\")\nax.set_xlabel(\"Sii\")\nax.set_ylabel(\"Count\")\nfor p, percentage in zip(ax.patches, percentages):\n    ax.annotate(f\"{percentage:.1f}%\", \n                (p.get_x() + p.get_width() / 2., p.get_height()), \n                ha='center', va='bottom', fontsize=10, color='black')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Bảng thể hiện đặc trưng tương quan với PCIAT_Total","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/train.csv')\nPCIAT_cols = [val for val in train.columns[train.columns.str.contains('PCIAT')]]\ntrain_cat_columns = train.select_dtypes(exclude='number').columns\nPCIAT_cols.remove('PCIAT-PCIAT_Total') \ntrain = train.drop(columns = PCIAT_cols)\ntrain = train.dropna(subset=['sii'])\nnumeric_cols = train.select_dtypes(include='number')\ncorr = numeric_cols.corr()['PCIAT-PCIAT_Total'].sort_values(ascending=False)\ncorr_df = corr.reset_index()\ncorr_df.columns = ['Feature', 'Correlation']\nplt.figure(figsize=(10, 10))\nsns.barplot(x='Correlation', y='Feature', data=corr_df, palette='coolwarm')\nfor index, row in corr_df.iterrows():\n    plt.text(row['Correlation'] + 0.01, index, f\"{row['Correlation']:.2f}\", va='center')\nplt.title('Các Đặc Trưng Tương Quan với PCIAT-PCIAT_Total', fontsize=16)\nplt.xlabel('Hệ số tương quan', fontsize=14)\nplt.ylabel('Đặc trưng', fontsize=14)\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/train.csv')\n\nmissing_train = train.isnull().mean() * 100\nmissing_train = missing_train.sort_values(ascending=False)\n\n# Chỉ chọn các feature có tỉ lệ thiếu cao (ví dụ > 5%)\nmissing_train_filtered = missing_train[missing_train > 5]\n\nplt.figure(figsize=(12, 20))\nsns.barplot(x=missing_train_filtered.values, y=missing_train_filtered.index, palette='viridis')\nplt.xlabel('Percentage of Missing Values (%)')\nplt.ylabel('Features')\nplt.title('Percentage of Missing Values per Feature in Training Set')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Version 1","metadata":{}},{"cell_type":"code","source":"#Model 1\n\n# Tải dữ liệu\ntrain = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/train.csv')\ntest = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/test.csv')\nsample = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/sample_submission.csv')\n\n# Tải và xử lý dữ liệu time series\ntrain_ts = load_time_series(\"/kaggle/input/child-mind-institute-problematic-internet-use/series_train.parquet\")\ntest_ts = load_time_series(\"/kaggle/input/child-mind-institute-problematic-internet-use/series_test.parquet\")\n\n# Thêm các cột mã hóa thời gian vào dữ liệu chính\ntime_series_cols = train_ts.columns.tolist()\ntime_series_cols.remove(\"id\")\n\n# Merge dữ liệu time series đã mã hóa với dữ liệu chính\ntrain = pd.merge(train, train_ts, how=\"left\", on='id')\ntest = pd.merge(test, test_ts, how=\"left\", on='id')\n\ntrain = train.drop('id', axis=1)\ntest = test.drop('id', axis=1)\n\n# Chọn cột đặc trưng\nfeaturesCols = ['Basic_Demos-Enroll_Season', 'Basic_Demos-Age', 'Basic_Demos-Sex',\n                'CGAS-Season', 'CGAS-CGAS_Score', 'Physical-Season', 'Physical-BMI',\n                'Physical-Height', 'Physical-Weight', 'Physical-Waist_Circumference',\n                'Physical-Diastolic_BP', 'Physical-HeartRate', 'Physical-Systolic_BP',\n                'Fitness_Endurance-Season', 'Fitness_Endurance-Max_Stage',\n                'Fitness_Endurance-Time_Mins', 'Fitness_Endurance-Time_Sec',\n                'FGC-Season', 'FGC-FGC_CU', 'FGC-FGC_CU_Zone', 'FGC-FGC_GSND',\n                'FGC-FGC_GSND_Zone', 'FGC-FGC_GSD', 'FGC-FGC_GSD_Zone', 'FGC-FGC_PU',\n                'FGC-FGC_PU_Zone', 'FGC-FGC_SRL', 'FGC-FGC_SRL_Zone', 'FGC-FGC_SRR',\n                'FGC-FGC_SRR_Zone', 'FGC-FGC_TL', 'FGC-FGC_TL_Zone', 'BIA-Season',\n                'BIA-BIA_Activity_Level_num', 'BIA-BIA_BMC', 'BIA-BIA_BMI',\n                'BIA-BIA_BMR', 'BIA-BIA_DEE', 'BIA-BIA_ECW', 'BIA-BIA_FFM',\n                'BIA-BIA_FFMI', 'BIA-BIA_FMI', 'BIA-BIA_Fat', 'BIA-BIA_Frame_num',\n                'BIA-BIA_ICW', 'BIA-BIA_LDM', 'BIA-BIA_LST', 'BIA-BIA_SMM',\n                'BIA-BIA_TBW', 'PAQ_A-Season', 'PAQ_A-PAQ_A_Total', 'PAQ_C-Season',\n                'PAQ_C-PAQ_C_Total', 'SDS-Season', 'SDS-SDS_Total_Raw',\n                'SDS-SDS_Total_T', 'PreInt_EduHx-Season',\n                'PreInt_EduHx-computerinternet_hoursday', 'sii']\n\nfeaturesCols += time_series_cols\n\n# Chọn các cột đặc trưng\ntrain = train[featuresCols]\ntrain = train.dropna(subset=['sii'])\n\n# Định nghĩa các cột mùa\ncat_c = [\n    'Basic_Demos-Enroll_Season', 'CGAS-Season', 'Physical-Season', \n    'Fitness_Endurance-Season', 'FGC-Season', 'BIA-Season', \n    'PAQ_A-Season', 'PAQ_C-Season', 'SDS-Season', 'PreInt_EduHx-Season'\n]\n\n# Gán các giá trị cột mùa bị thiếu bằng Missing\ndef update(df):\n    global cat_c\n    for c in cat_c: \n        df[c] = df[c].fillna('Missing')\n        df[c] = df[c].astype('category')\n    return df\n        \ntrain = update(train)\ntest = update(test)\n\n# Chuyển các dữ liệu sang số\nseason_mapping = {'Spring': 1, 'Summer': 2, 'Fall': 3, 'Winter': 4}\nfor col in cat_c:\n    train[col] = train[col].map(season_mapping)\n    test[col] = test[col].map(season_mapping)\n\nimputer = SimpleImputer(strategy='median')\n\n# Tạo các mô hình\nensemble = VotingRegressor(estimators=[\n    ('lgb', Pipeline(steps=[('imputer', imputer), ('regressor', LGBMRegressor(random_state=SEED))])),\n    ('xgb', Pipeline(steps=[('imputer', imputer), ('regressor', XGBRegressor(random_state=SEED))])),\n    ('cat', Pipeline(steps=[('imputer', imputer), ('regressor', CatBoostRegressor(random_state=SEED))])),\n])\n\n# Train the ensemble model\nSubmission1 = TrainModel(ensemble, test)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Version 2","metadata":{}},{"cell_type":"code","source":"#Model 2\n\n# Tải dữ liệu\ntrain = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/train.csv')\ntest = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/test.csv')\nsample = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/sample_submission.csv')\n\n# Tải và xử lý dữ liệu time series\ntrain_ts = load_time_series(\"/kaggle/input/child-mind-institute-problematic-internet-use/series_train.parquet\")\ntest_ts = load_time_series(\"/kaggle/input/child-mind-institute-problematic-internet-use/series_test.parquet\")\n\n# Thêm các cột mã hóa thời gian vào dữ liệu chính\ntime_series_cols = train_ts.columns.tolist()\ntime_series_cols.remove(\"id\")\n\n# Merge dữ liệu time series đã mã hóa với dữ liệu chính\ntrain = pd.merge(train, train_ts, how=\"left\", on='id')\ntest = pd.merge(test, test_ts, how=\"left\", on='id')\n\ntrain = train.drop('id', axis=1)\ntest = test.drop('id', axis=1)\n\n# Chọn cột đặc trưng\nfeaturesCols = ['Basic_Demos-Enroll_Season', 'Basic_Demos-Age', 'Basic_Demos-Sex',\n                'CGAS-Season', 'CGAS-CGAS_Score', 'Physical-Season', 'Physical-BMI',\n                'Physical-Height', 'Physical-Weight', 'Physical-Waist_Circumference',\n                'Physical-Diastolic_BP', 'Physical-HeartRate', 'Physical-Systolic_BP',\n                'Fitness_Endurance-Season', 'Fitness_Endurance-Max_Stage',\n                'Fitness_Endurance-Time_Mins', 'Fitness_Endurance-Time_Sec',\n                'FGC-Season', 'FGC-FGC_CU', 'FGC-FGC_CU_Zone', 'FGC-FGC_GSND',\n                'FGC-FGC_GSND_Zone', 'FGC-FGC_GSD', 'FGC-FGC_GSD_Zone', 'FGC-FGC_PU',\n                'FGC-FGC_PU_Zone', 'FGC-FGC_SRL', 'FGC-FGC_SRL_Zone', 'FGC-FGC_SRR',\n                'FGC-FGC_SRR_Zone', 'FGC-FGC_TL', 'FGC-FGC_TL_Zone', 'BIA-Season',\n                'BIA-BIA_Activity_Level_num', 'BIA-BIA_BMC', 'BIA-BIA_BMI',\n                'BIA-BIA_BMR', 'BIA-BIA_DEE', 'BIA-BIA_ECW', 'BIA-BIA_FFM',\n                'BIA-BIA_FFMI', 'BIA-BIA_FMI', 'BIA-BIA_Fat', 'BIA-BIA_Frame_num',\n                'BIA-BIA_ICW', 'BIA-BIA_LDM', 'BIA-BIA_LST', 'BIA-BIA_SMM',\n                'BIA-BIA_TBW', 'PAQ_A-Season', 'PAQ_A-PAQ_A_Total', 'PAQ_C-Season',\n                'PAQ_C-PAQ_C_Total', 'SDS-Season', 'SDS-SDS_Total_Raw',\n                'SDS-SDS_Total_T', 'PreInt_EduHx-Season',\n                'PreInt_EduHx-computerinternet_hoursday']\n\nfeaturesCols += time_series_cols\n\n# Đảm bảo 'sii' nằm trong featuresCols trước khi chọn cột\nfeaturesCols.append('sii')\n\n# Chọn các cột đặc trưng\ntrain = train[featuresCols]\ntrain = train.dropna(subset=['sii'])\n\n# Định nghĩa các cột phân loại\ncat_c = [\n    'Basic_Demos-Enroll_Season', 'CGAS-Season', 'Physical-Season', \n    'Fitness_Endurance-Season', 'FGC-Season', 'BIA-Season', \n    'PAQ_A-Season', 'PAQ_C-Season', 'SDS-Season', 'PreInt_EduHx-Season'\n]\n# Cập nhật các cột phân loại\ndef update(df):\n    global cat_c\n    for c in cat_c: \n        df[c] = df[c].fillna('Missing')\n        df[c] = df[c].astype('category')\n    return df\n        \ntrain = update(train)\ntest = update(test)\n\n# Mã hóa các cột phân loại thành số nguyên\ndef create_mapping(column, dataset):\n    unique_values = dataset[column].unique()\n    return {value: idx for idx, value in enumerate(unique_values)}\n\nfor col in cat_c:\n    mapping = create_mapping(col, train)\n    mappingTe = create_mapping(col, test)\n    \n    train[col] = train[col].replace(mapping).astype(int)\n    test[col] = test[col].replace(mappingTe).astype(int)\n\n# Định nghĩa các tham số cho các mô hình\nlgb_Params = {\n    'learning_rate': 0.046,\n    'max_depth': 9,\n    'num_leaves': 478,\n    'min_data_in_leaf': 13,\n    'feature_fraction': 0.9,\n    'bagging_fraction': 0.8,\n    'bagging_freq': 4,\n    'verbose': -1,\n    'lambda_l1': 10,\n    'lambda_l2': 0.01,\n}\n\nXGB_Params = {\n    'max_depth': 6,\n    'n_estimators': 150,\n    'learning_rate': 0.05,\n    'subsample': 0.6,\n    'colsample_bytree': 0.8,\n    'reg_alpha': 1,\n    'reg_lambda': 5,\n    'random_state': SEED,\n    'enable_categorical': True\n}\n\nCatBoost_Params = {\n    'learning_rate': 0.05,\n    'depth': 6,\n    'iterations': 200,\n    'random_seed': SEED,\n    'cat_features': cat_c,\n    'verbose': 0,\n    'l2_leaf_reg': 10\n}\n\n# Tạo các mô hình\nLight = LGBMRegressor(**lgb_Params)\nXGB_Model = XGBRegressor(**XGB_Params)\nCatBoost_Model = CatBoostRegressor(**CatBoost_Params)\n\n# Tạo mô hình Voting Regressor\nvoting_model = VotingRegressor(estimators=[\n    ('lightgbm', Light),\n    ('xgboost', XGB_Model),\n    ('catboost', CatBoost_Model),\n]) \n\n# Train the ensemble model\nSubmission2 = TrainModel(voting_model, test)","metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Version 3","metadata":{}},{"cell_type":"code","source":"# Tải dữ liệu\ntrain = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/train.csv')\ntest = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/test.csv')\nsample = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/sample_submission.csv')\n\ntrain_ts = load_time_series(\"/kaggle/input/child-mind-institute-problematic-internet-use/series_train.parquet\")\ntest_ts = load_time_series(\"/kaggle/input/child-mind-institute-problematic-internet-use/series_test.parquet\")\n\ndf_train = train_ts.drop('id', axis=1)\ndf_test = test_ts.drop('id', axis=1)\n\ntrain_ts_encoded = perform_autoencoder(df_train, encoding_dim=60, epochs=100, batch_size=32)\ntest_ts_encoded = perform_autoencoder(df_test, encoding_dim=60, epochs=100, batch_size=32)\n\n# Thêm các cột mã hóa thời gian vào dữ liệu chính\ntime_series_cols = train_ts_encoded.columns.tolist()\ntrain_ts_encoded[\"id\"]=train_ts[\"id\"]\ntest_ts_encoded['id']=test_ts[\"id\"]\n\n# Merge dữ liệu time series với dữ liệu chính\ntrain = pd.merge(train, train_ts_encoded, how=\"left\", on='id')\ntest = pd.merge(test, test_ts_encoded, how=\"left\", on='id')\n\n# Impute dữ liệu numeric bằng KNNImputer\nknn_imputer = KNNImputer(n_neighbors=5)\nnumeric_cols = train.select_dtypes(include=['int32', 'int64', 'float64']).columns\nimputed_data = knn_imputer.fit_transform(train[numeric_cols])\ntrain_imputed = pd.DataFrame(imputed_data, columns=numeric_cols)\ntrain_imputed['sii'] = train_imputed['sii'].round().astype(int)\n\nfor col in train.columns:\n    if col not in numeric_cols:\n        train_imputed[col] = train[col]\n\ntrain = train_imputed\n\n# Áp dụng feature engineering\ntrain = feature_engineering(train)\ntest = feature_engineering(test)\n\nif 'id' in train.columns:\n    train = train.drop('id', axis=1)\nif 'id' in test.columns:\n    test = test.drop('id', axis=1)\n\n# Chọn cột đặc trưng\nfeaturesCols = [\n    'Basic_Demos-Age', 'Basic_Demos-Sex',\n    'CGAS-CGAS_Score', 'Physical-BMI',\n    'Physical-Height', 'Physical-Weight', 'Physical-Waist_Circumference',\n    'Physical-Diastolic_BP', 'Physical-HeartRate', 'Physical-Systolic_BP',\n    'Fitness_Endurance-Max_Stage',\n    'Fitness_Endurance-Time_Mins', 'Fitness_Endurance-Time_Sec',\n    'FGC-FGC_CU', 'FGC-FGC_CU_Zone', 'FGC-FGC_GSND',\n    'FGC-FGC_GSND_Zone', 'FGC-FGC_GSD', 'FGC-FGC_GSD_Zone', 'FGC-FGC_PU',\n    'FGC-FGC_PU_Zone', 'FGC-FGC_SRL', 'FGC-FGC_SRL_Zone', 'FGC-FGC_SRR',\n    'FGC-FGC_SRR_Zone', 'FGC-FGC_TL', 'FGC-FGC_TL_Zone',\n    'BIA-BIA_Activity_Level_num', 'BIA-BIA_BMC', 'BIA-BIA_BMI',\n    'BIA-BIA_BMR', 'BIA-BIA_DEE', 'BIA-BIA_ECW', 'BIA-BIA_FFM',\n    'BIA-BIA_FFMI', 'BIA-BIA_FMI', 'BIA-BIA_Fat', 'BIA-BIA_Frame_num',\n    'BIA-BIA_ICW', 'BIA-BIA_LDM', 'BIA-BIA_LST', 'BIA-BIA_SMM',\n    'BIA-BIA_TBW', 'PAQ_A-PAQ_A_Total',\n    'PAQ_C-PAQ_C_Total', 'SDS-SDS_Total_Raw',\n    'SDS-SDS_Total_T',\n    'PreInt_EduHx-computerinternet_hoursday', 'BMI_Age', 'Internet_Hours_Age', 'BMI_Internet_Hours',\n    'BFP_BMI', 'FFMI_BFP', 'FMI_BFP', 'LST_TBW', 'BFP_BMR', 'BFP_DEE', 'BMR_Weight', 'DEE_Weight',\n    'SMM_Height', 'Muscle_to_Fat', 'Hydration_Status', 'ICW_TBW'\n]\n\n\n\n# Thêm các cột từ time series\nfeaturesCols += time_series_cols\nfeaturesCols.append('sii')\n\ntrain = train[featuresCols]\ntrain = train.dropna(subset='sii')\n\n# Tương tự cho test\nfeaturesCols = [c for c in featuresCols if c != 'sii']\ntest = test[featuresCols]\n\n# Kiểm tra và xử lý các giá trị vô hạn trong dữ liệu huấn luyện\nif np.any(np.isinf(train)):\n    train = train.replace([np.inf, -np.inf], np.nan)\n\n# Định nghĩa các tham số cho các mô hình\nlgb_Params = {\n    'learning_rate': 0.046,\n    'max_depth': 9,\n    'num_leaves': 478,\n    'min_data_in_leaf': 13,\n    'feature_fraction': 0.9,\n    'bagging_fraction': 0.8,\n    'bagging_freq': 4,\n    'verbose': -1,\n    'lambda_l1': 10,\n    'lambda_l2': 0.01,\n}\n\nXGB_Params = {\n    'max_depth': 6,\n    'n_estimators': 150,\n    'learning_rate': 0.05,\n    'subsample': 0.6,\n    'colsample_bytree': 0.8,\n    'reg_alpha': 1,\n    'reg_lambda': 5,\n    'random_state': SEED\n}\n\nCatBoost_Params = {\n    'learning_rate': 0.05,\n    'depth': 6,\n    'iterations': 200,\n    'random_seed': SEED,\n    'verbose': 0,\n    'l2_leaf_reg': 10\n}\n\n# Tạo các mô hình\nLight = LGBMRegressor(**lgb_Params)\nXGB_Model = XGBRegressor(**XGB_Params)\nCatBoost_Model = CatBoostRegressor(**CatBoost_Params)\n\n# Tạo mô hình Voting Regressor\nvoting_model = VotingRegressor(estimators=[\n    ('lightgbm', Light),\n    ('xgboost', XGB_Model),\n    ('catboost', CatBoost_Model),\n]) \n\n# Train the ensemble model\nSubmission3= TrainModel(voting_model, test)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Kết quả","metadata":{}},{"cell_type":"code","source":"sub1 = Submission1\nsub1 = sub1.sort_values(by='id').reset_index(drop=True)\n\nsub2 = Submission2\nsub2 = sub2.sort_values(by='id').reset_index(drop=True)\n\nsub3 = Submission3\nsub3 = sub3.sort_values(by='id').reset_index(drop=True)\n\ncombined = pd.DataFrame({\n    'id': sub1['id'],\n    'sii_1': sub1['sii'],\n    'sii_2': sub2['sii'],\n    'sii_3': sub3['sii'],\n})\n\ndef majority_vote(row):\n    return row.mode()[0]\n\ncombined['final_sii'] = combined[['sii_1', 'sii_2', 'sii_3']].apply(majority_vote, axis=1)\n\nfinal_submission = combined[['id', 'final_sii']].rename(columns={'final_sii': 'sii'})\n\n# Save submission\nfinal_submission.to_csv('submission.csv', index=False)\nprint(\"Save submission!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}